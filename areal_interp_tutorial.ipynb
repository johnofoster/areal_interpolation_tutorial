{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8105db",
   "metadata": {},
   "source": [
    "# Areal Interpolation in Python using Tobler - A Jupyter Notebook Tutorial\n",
    "# Tutorial\n",
    "\n",
    "If you are viewing this tutorial but for some reason have not seen the readme file be sure to check that out [here](https://github.com/johnofoster/areal_interpolation_tutorial).\n",
    "\n",
    "Ok let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e4e96b-6307-4a40-8172-8bd7605d4406",
   "metadata": {},
   "source": [
    "### Import Modules and Packages\n",
    "\n",
    "This tutorial requires the use of a number of modules. Tobler, Pandas, GeoPandas, and Plotly will perform most of the work but see the comments below for why we are importing each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540f184-d595-4ca9-81c6-22448e9eef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "# --------------\n",
    "\n",
    "import geopandas as gpd                     # For operations on GeoDataFrames\n",
    "from IPython.display import display_html    # To display DataFrames side-by-side\n",
    "import json                                 # For plotting geometries with Plotly\n",
    "import matplotlib.pyplot as plt             # General purpose plotting\n",
    "import numpy as np                          # Statistical functions and classes\n",
    "import pandas as pd                         # For operations on DataFrames\n",
    "import plotly.express as px                 # For interactive plotting\n",
    "import tobler                               # For the areal interpolation functions\n",
    "from scipy import stats                     # For the linear regression function\n",
    "from shapely.geometry import Point          # For the point geometry class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b7e9c",
   "metadata": {},
   "source": [
    "## Part 1 - Areal Interpolation of Census Population Data\n",
    "\n",
    "In part 1 we will perform the areal interpolation of Ottawa, ON population data. In an effort to understand how the results are affected by different interpolation methods and different spatial resolutions we will perform four sets of areal interpolation.\n",
    "\n",
    "* Census tracts to neighborhoods using area weighted interpolation\n",
    "* Dissemination areas to neighborhoods using area weighted interpolation\n",
    "* Census tracts to neighborhoods using dasymetric interpolation\n",
    "* Dissemination areas to neighborhoods using dasymetric interpolation\n",
    "\n",
    "Census tracts are census geographic areas that typically contain about 2500 to 10000 people whereas dissemination areas are much smaller and contain about 400 to 700 people. Read about them and other census geographic areas [here](https://www12.statcan.gc.ca/census-recensement/2016/ref/98-304/chap12-eng.cfm)\n",
    "\n",
    "- [tobler.area_weighted.area_interpolate](https://pysal.org/tobler/generated/tobler.area_weighted.area_interpolate.html#tobler.area_weighted.area_interpolate)\n",
    "- [tobler.dasymetric.masked_area_interpolate](https://pysal.org/tobler/generated/tobler.dasymetric.masked_area_interpolate.html#tobler.dasymetric.masked_area_interpolate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4b0cf4-4fab-4f65-aea4-dce41f1c435e",
   "metadata": {},
   "source": [
    "### Read The Data\n",
    "\n",
    "First, we need read the data from our `data/` directory and assign them to variables. The Ottawa census tracts data, dissemination tracts data, and neighborhoods data have been prepared in advance so the tutorial can focus on the interpolation and assessment procedures.\n",
    "\n",
    "**Important Note about coordinate reference systems:** These three spatial dataset have already been reprojected into the WGS 84 / UTM 18N (EPSG:32618) CRS. If you use Tobler with your own GeoDataFrames it's important to explicitly reproject the data into an appropriate UTM zone, even though Tobler will try to do this automatically. Reprojecting a GeoDataFrame is easily accomplished with the [`gdf.to_crs()`](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.to_crs.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001fae5c-1316-4e0a-acc9-baf0ad5ff5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "# -------------\n",
    "\n",
    "# City of Ottawa census tracts data\n",
    "ct_gdf = gpd.read_file(filename='data/ottawa_ct_pop_2016.gpkg')\n",
    "\n",
    "# City of Ottawa dissemination areas data\n",
    "da_gdf = gpd.read_file(filename='data/ottawa_da_pop_2016.gpkg')\n",
    "\n",
    "# City of Ottawa neighborhoods data\n",
    "nbhd_gdf = gpd.read_file(filename='data/ottawa_neighborhoods.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a8f50",
   "metadata": {},
   "source": [
    "### Inspect The Data\n",
    "\n",
    "The following Pandas and GeoPandas methods are great ways to inspect DataFrames and GeoDataFrames. Note: The DataFrame (`df`) methods also work on GeoDataFrames (`gdf`) but `explore()` only works with GeoDataFrames. Follow their links to see the documentation as their outputs can be controlled by passing different arguments to them.\n",
    "\n",
    "* [`df.info()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html): Information about the DataFrame/GeoDataFrame such as column names, data types, and length\n",
    "* [`df.head()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html): First n rows of the DataFrame/GeoDataFrame (defaults to 10 rows)\n",
    "* [`df.tail()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.tail.html): Last n rows of the DataFrame/GeoDataFrame (defaults to 10 rows) \n",
    "* [`df.sample()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html): Random sample of n rows of the DataFrame/GeoDataFrame (defaults to 1 rows)\n",
    "* [`gdf.explore()`](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html): Interactive map of a GeoDataFrame\n",
    "* [`gdf.crs`](https://geopandas.org/en/stable/docs/user_guide/projections.html): Coordinate reference system information of a GeoDataFrame\n",
    "\n",
    "\n",
    "The cell below contains these data inspection methods. Just replace `df` or `gdf` before the dot notation with the DataFrame or GeoDataFrame's name that you want to inspect. For example:\n",
    "* `ct_gdf.info()` - shows info about the census tracts GeoDataFrame\n",
    "* `da_gdf.explore(column='da_pop_2016')` - displays an interactive map of the dissemination areas with a colormap of the population column\n",
    "\n",
    "Alternatively, add in some new code cells in order to run them separately. And feel free to insert or split cells at any point in this tutorial in order to inspect any of the intermediate data. These methods will really help to reveal exactly what is going. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d85ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data\n",
    "# ----------------\n",
    "\n",
    "# Uncomment individual lines below and replace `df` or `gdf` with the name of a variable\n",
    "# DataFrame (df) methods work on GeoDataFrames (gdf) but not the other way around\n",
    "\n",
    "# df.info()\n",
    "# df.head()\n",
    "# df.tail(5)\n",
    "# df.sample(15)\n",
    "# gdf.explore()\n",
    "# gdf.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a8cc2",
   "metadata": {},
   "source": [
    "First, using the `df.sample()` method, we'll look at small random samples of the census tracts, dissemination areas, and neighborhoods GeoDataFrames while ignoring their geometry columns. This will show us the column names and give us some examples of how the values of each are formatted. Because the output of cells in Jupyter notebooks are rendered in HTML we can use Pandas' DataFrame styling methods to display several DataFrames side-by-side. While this takes more code than simply using the `print()` function or displaying the results of `sample()`, it facilitates better comparisons and saves vertical space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12685948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare random samples from each GeoDataFrame\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Assign a sampling of each GeoDataFrame to a DataFrame\n",
    "df1 = ct_gdf.drop(columns='geometry').sample(5)\n",
    "df2 = da_gdf.drop(columns='geometry').sample(5)\n",
    "df3 = nbhd_gdf.drop(columns='geometry').sample(5)\n",
    "\n",
    "# Style the DataFrames and include captions with the number of rows\n",
    "style = \"style='display:inline; padding:10px'\"\n",
    "df1_styled = df1.style.set_table_attributes(style).set_caption(f'Census Tracts, {len(ct_gdf)} rows')\n",
    "df2_styled = df2.style.set_table_attributes(style).set_caption(f'Dissemination Areas, {len(da_gdf)} rows')\n",
    "df3_styled = df3.style.set_table_attributes(style).set_caption(f'Neighborhoods, {len(nbhd_gdf)} rows')\n",
    "\n",
    "# Display the three DataFrames\n",
    "display_html(df1_styled._repr_html_() + df2_styled._repr_html_() + df3_styled._repr_html_(), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febcfed2",
   "metadata": {},
   "source": [
    "Now let's plot the geometries of each GeoDataFrame next to each other using GeoPandas' Matplotlib `plot()` method. This will reveal the scale and shape of the geometries that we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the target and source geometries\n",
    "# -------------------------------------\n",
    "\n",
    "# Create subplots; set their size and padding\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(25, 25))\n",
    "plt.subplots_adjust(wspace=0)\n",
    "\n",
    "# Plot the Ottawa census tracts figure\n",
    "ct_gdf.plot(ax=axs[0], color=\"none\", linewidth=0.25)\n",
    "axs[0].set_title('Source 1: Ottawa Census Tracts')\n",
    "axs[0].axis('off')\n",
    "\n",
    "# Ottawa dissemination areas figure\n",
    "da_gdf.plot(ax=axs[1], color=\"none\", linewidth=0.25)\n",
    "axs[1].set_title('Source 2: Ottawa Dissemination Areas')\n",
    "axs[1].axis('off')\n",
    "\n",
    "# Ottawa neighborhoods figure\n",
    "nbhd_gdf.plot(ax=axs[2], color=\"none\", linewidth=0.25)\n",
    "axs[2].set_title('Target: Ottawa Neighborhoods')\n",
    "axs[2].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3db67",
   "metadata": {},
   "source": [
    "### Areal Interpolation of Census Data\n",
    "\n",
    "#### Area Weighted Interpolation of Census Tracts and Dissemination Areas\n",
    "\n",
    "Now that we have an understanding of the data we're working with we can move on to the area weighted and dasymetric interpolation.\n",
    "\n",
    "Area weighted interpolation works by intersecting the source and target geometries, calculating the areal weights (i.e. the proportion a source geometry is covered by each given target geometry), and then allocating a proportional amount of the variable from the source to the targets.\n",
    "\n",
    "Please refer to Tobler's documentation of the [`tobler.area_weighted.area_interpolate()`](https://pysal.org/tobler/generated/tobler.area_weighted.area_interpolate.html) function for a mathematical description and help with the parameters.\n",
    "\n",
    "The first thing to consider is whether the variable to be interpolated is extensive or intensive. An extensive variable is one that depends on the size of the sample (e.g. population counts) while an intensive variable does not (e.g. population density). In this tutorial we will only be dealing with extensive variables (population counts) but it's worth noting that Tobler's interpolation methods can handle both types through the use of different parameters.\n",
    "\n",
    "We will pass source and target GeoDataFrames to the function along with the column name of the extensive variable. The extensive variable will then be interpolated from the source geometry to the target geometry. The result will be GeoDataFrame containing the target geometries and interpolated values of the extensive variable.\n",
    "\n",
    "We will perform the areal interpolation twice: first with the census tracts as the source data, and then with the dissemination areas. This will allow us to compare the results of these different source geometries and see if there's any benefit to using smaller census geographies (i.e. dissemination areas) as the source.\n",
    "\n",
    "After running these cells there will be no output but the results will be saved to the `ct_area_interp_gdf` and `da_area_interp_gdf` variables. We'll plot them once all four interpolation operations have been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5bcd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area weighted interpolation: census tracts to neighborhoods\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "ct_area_interp_gdf = tobler.area_weighted.area_interpolate(source_df=ct_gdf, \n",
    "                                                        target_df=nbhd_gdf,\n",
    "                                                        extensive_variables=['ct_pop_2016'])\n",
    "\n",
    "# Round the interpolation results to the nearest integer and change the type to integer\n",
    "# (Population counts must be integers)\n",
    "ct_area_interp_gdf['ct_pop_2016'] = ct_area_interp_gdf['ct_pop_2016'].round(decimals=0).astype(int)\n",
    "\n",
    "# Rename the results column for clarity later\n",
    "ct_area_interp_gdf = ct_area_interp_gdf.rename(columns={'ct_pop_2016':'ct_area_interp_est'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab1d548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area weighted interpolation: dissemination areas to neighborhoods\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "da_area_interp_gdf = tobler.area_weighted.area_interpolate(source_df=da_gdf, \n",
    "                                                        target_df=nbhd_gdf,\n",
    "                                                        extensive_variables=['da_pop_2016'])\n",
    "\n",
    "# Round the interpolation results to the nearest integer and change the type to integer\n",
    "# (Population counts must be integers)\n",
    "da_area_interp_gdf['da_pop_2016'] = da_area_interp_gdf['da_pop_2016'].round(decimals=0).astype(int)\n",
    "\n",
    "# Rename the results column for clarity later\n",
    "da_area_interp_gdf = da_area_interp_gdf.rename(columns={'da_pop_2016':'da_area_interp_est'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c5b76",
   "metadata": {},
   "source": [
    "#### Dasymetric Interpolation of Census Tracts and Dissemination Areas\n",
    "\n",
    "With that complete we can move on the dasymetric interpolation.\n",
    "\n",
    "Dasymetric interpolation, or masked area weighted interpolation, incorporates secondary information to help refine the spatial distribution of the variable. For example, a census tract might include an inhabited urban area and a large uninhabited park. Secondary information about that census tract's land cover is used to mask out those parts of the census tract that don't have an urban land cover, and thus don't contain the extensive variable. Area weighted interpolation is then applied to these masked census tracts and the results should be more accurate than simple area weighted interpolation on its own.\n",
    "\n",
    "Please refer to Tobler's documentation of the `tobler.dasymetric.masked_area_interpolate()`](https://pysal.org/tobler/generated/tobler.dasymetric.masked_area_interpolate.html#tobler.dasymetric.masked_area_interpolate) function for help with the parameters.\n",
    "\n",
    "The land cover raster that we will use comes from the [2015 Land Cover of Canada](https://open.canada.ca/data/en/dataset/4e615eae-b90c-420b-adee-2ca35896caf6). This 30 m spatial resolution land cover product is produced every 5 years and covers the whole country. Tobler's dasymetric interpolation function will automatically clip the raster to the extent of the source data (City of Ottawa) but I have already clipped it to reduce the file-size and processing time.\n",
    "\n",
    "The path of the land cover raster and the pixel value(s) of the land cover classes that contain dwellings are passed to the function along with the source and target GeoDataFrames, and the column name of the extensive variable. The result is a GeoDataFrame made up of the target geometries with interpolated values for the extensive variable. \n",
    "\n",
    "For the same reason as before, we will perform the dasymetric interpolation twice: first with the census tracts as the source data, and then with the dissemination areas. `masked_area_interpolate()` will throw some warnings but just ignore them. The length of time it takes to run each of these cells on my computer has been included for your reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad844b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dasymetric interpolation: census tracts + urban landover to neighborhoods\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "# Perform dasymetric interpolation\n",
    "ct_dasy_interp_gdf = tobler.dasymetric.masked_area_interpolate(source_df=ct_gdf, \n",
    "                                                            target_df=nbhd_gdf,\n",
    "                                                            raster='data/ottawa_landcover.tif',\n",
    "                                                            codes=[17],\n",
    "                                                            extensive_variables=['ct_pop_2016'])\n",
    "\n",
    "# Round the interpolation results to the nearest integer and change the type to integer\n",
    "# (Population counts must be integers)\n",
    "ct_dasy_interp_gdf['ct_pop_2016'] = ct_dasy_interp_gdf['ct_pop_2016'].round(decimals=0).astype(int)\n",
    "\n",
    "# Rename the results column for clarity later\n",
    "ct_dasy_interp_gdf = ct_dasy_interp_gdf.rename(columns={'ct_pop_2016':'ct_dasy_interp_est'})\n",
    "\n",
    "# ~30 s ; ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dasymetric interpolation: dissemination areas + urban landover to neighborhoods\n",
    "#------------------------------------------------------------------------\n",
    "\n",
    "# Perform dasymetric interpolation\n",
    "da_dasy_interp_gdf = tobler.dasymetric.masked_area_interpolate(source_df=da_gdf, \n",
    "                                                            target_df=nbhd_gdf,\n",
    "                                                            raster='data/ottawa_landcover.tif',\n",
    "                                                            codes=[17],\n",
    "                                                            extensive_variables=['da_pop_2016'])\n",
    "\n",
    "# Round the interpolation results to the nearest integer and change the type to integer\n",
    "# (Population counts must be integers)\n",
    "da_dasy_interp_gdf['da_pop_2016'] = da_dasy_interp_gdf['da_pop_2016'].round(decimals=0).astype(int)\n",
    "\n",
    "# Rename the results column for clarity later\n",
    "da_dasy_interp_gdf = da_dasy_interp_gdf.rename(columns={'da_pop_2016':'da_dasy_interp_est'})\n",
    "\n",
    "# ~1.5 mins : ignore warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58bff6b",
   "metadata": {},
   "source": [
    "### Assess the Interpolation Results\n",
    "\n",
    "Because the Ottawa Neighbourhood Study data came with a population estimate for each neighborhood (`nbhd_pop_est`) we can compare those estimated values against the interpolated values. That being said, it's important to note that the accuracy of this assessment will be very limited for a number of reasons:\n",
    "\n",
    "1. Metadata associated with the Ottawa neighborhood survey data does not specify the year of their population estimates. This data was first published to the Open Ottawa data portal on November 25, 2019 and then updated on June 2, 2021. The census tract and dissemination area population data comes from the 2016 census.\n",
    "2. Metadata associated with the Ottawa neighborhood survey data does not specify the methodology they used for estimating the neighborhood populations. Perhaps it came from an areal interpolation method similar to what we are using or perhaps it came from a higher resolution data set that the City of Ottawa has but which they don't publish (e.g. size of each households).\n",
    "3. The Ottawa neighborhood survey data indicates that the 'Beechwood Cemetery' and 'Notre-Dame Cemetery' neighborhoods have populations of 139 and 17, respectively, despite no evidence of residences within them.\n",
    "4. The sum of all the neighborhood population estimates is 867146 while the census_tracts' sum is 934243. These last two points suggest that these neighborhood population estimates are the result of interpolation from an unknown dataset (2011 census data perhaps?).\n",
    "\n",
    "Despite these issues we will go ahead and use these estimates to assess the interpolation results. We will do the following:\n",
    "\n",
    "1. Merge the interpolation results with the neighborhood population estimates and look at a sample of the results\n",
    "2. Perform a statistical assessment of the interpolation results\n",
    "3. Visually assess the percent error of each neighborhood (interpolated population vs `nbhd_pop_est`)\n",
    "4. Create a 1:1 scatter plot of the interpolation results against the neighborhood population estimates\n",
    "5. Discuss the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the results for comparison\n",
    "#---------------------------------\n",
    "\n",
    "# Create a list of the GeoDataFrames (dropping the redundant geometry)\n",
    "dfs = [nbhd_gdf,\n",
    "        ct_area_interp_gdf.drop(columns='geometry'),\n",
    "        ct_dasy_interp_gdf.drop(columns='geometry'),\n",
    "        da_area_interp_gdf.drop(columns='geometry'),\n",
    "        da_dasy_interp_gdf.drop(columns='geometry'),]\n",
    "\n",
    "# Concatenate the GeoDataFrames\n",
    "interp_results_gdf = pd.concat(objs=dfs, axis=1)\n",
    "\n",
    "# View a sample of the interpolation results GeoDataFrame (without the geometry column)\n",
    "interp_results_gdf.sample(10).drop(columns='geometry')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7146bc97",
   "metadata": {},
   "source": [
    "Looking at this sample we can see that the different interpolation methods and source data have yielded results that look roughly in line with the neighborhood population estimates that came from the Ottawa Neighbourhood Study data (`nbhd_pop_est`). \n",
    "\n",
    "Let's continue with our evaluation while not forgetting that the neighborhood population estimates have some issues. We will begin by defining some functions to calculate percent error, root mean square error, normalized square error, mean bias error, mean absolute error, and r value. There are 3rd party modules that contain functions to perform these calculations but defining our own is a good exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to assess the results\n",
    "# --------------------------------------\n",
    "\n",
    "# PE\n",
    "def percent_error(estimated, expected):\n",
    "    ''' Return Percent Error where estimated and expected are numeric or array-like'''\n",
    "    return abs(((estimated - expected) / expected) * 100)\n",
    "\n",
    "# RMSE\n",
    "def rmse(estimated, expected):\n",
    "    ''' Return Root Mean Square Error where estimated and expected are array-like'''\n",
    "    return np.sqrt(np.mean((estimated - expected) ** 2))\n",
    "\n",
    "# NRMSE\n",
    "def nrmse(estimated, expected):\n",
    "    ''' Return Normalized Root Mean Square Error where estimated and expected are array-like'''\n",
    "    return np.sqrt(np.mean((estimated - expected) ** 2))/np.std(estimated)\n",
    "\n",
    "# MBE\n",
    "def mbe(estimated, expected):\n",
    "    ''' Return Mean Bias Error where estimated and expected are array-like'''\n",
    "    return np.mean(estimated - expected)\n",
    "\n",
    "# MAE\n",
    "def mae(estimated, expected):\n",
    "    ''' Return Mean Mean Absolute Error where estimated and expected are array-like'''\n",
    "    return np.mean(abs(estimated - expected))\n",
    "\n",
    "# r value\n",
    "def r_value(estimated, expected):\n",
    "    ''' Return r value where estimated and expected are array-like.'''\n",
    "\n",
    "    # Get the r_value by unpacking the results from SciPy's linregress() function\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(estimated, expected)\n",
    "    return r_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79fc3a",
   "metadata": {},
   "source": [
    "We will now use those functions to generate statistics on the four sets of interpolation results. The percent error values are concatenated to the `interp_results_gdf` so we can plot maps showing the percent error of each neighborhood. The other statistics are placed into their own DataFrame so we can assess them in a tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics on the interpolation results\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Assign the interpolation results columns to their own variables for clarity\n",
    "ct_area_est = interp_results_gdf['ct_area_interp_est'] # Source: census tracts | method: area weighted\n",
    "ct_dasy_est = interp_results_gdf['ct_dasy_interp_est'] # Source: census tracts  method: dasymetric\n",
    "da_area_est = interp_results_gdf['da_area_interp_est'] # Source: dissemination areas | method: area weighted\n",
    "da_dasy_est = interp_results_gdf['da_dasy_interp_est'] # Source: dissemination areas | method: dasymetric\n",
    "expected = interp_results_gdf['nbhd_pop_est']          # Source: neighborhoods\n",
    "\n",
    "# Create new columns in interp_results_gdf to represent percent error\n",
    "interp_results_gdf['ct_area_interp_%_error'] = round(percent_error(ct_area_est, expected), 1)\n",
    "interp_results_gdf['ct_dasy_interp_%_error'] = round(percent_error(ct_dasy_est, expected), 1)\n",
    "interp_results_gdf['da_area_interp_%_error'] = round(percent_error(da_area_est, expected), 1)\n",
    "interp_results_gdf['da_dasy_interp_%_error'] = round(percent_error(da_dasy_est, expected), 1)\n",
    "\n",
    "# Create a DataFrame containing the other statistics\n",
    "interp_stats_df = pd.DataFrame(data={'Interpolation Method': ['Area Weighted','Area Weighted',\n",
    "                                                        'Dasymetric', 'Dasymetric'],\n",
    "                                'Source Geographies': ['Census Tracts', 'Dissemination Areas',\n",
    "                                                       'Census Tracts', 'Dissemination Areas'],\n",
    "                                'RMSE': [rmse(ct_area_est, expected),\n",
    "                                        rmse(da_area_est, expected),\n",
    "                                        rmse(ct_dasy_est, expected),\n",
    "                                        rmse(da_dasy_est, expected)],\n",
    "                                'NRMSE': [nrmse(ct_area_est, expected),\n",
    "                                        nrmse(da_area_est, expected),\n",
    "                                        nrmse(ct_dasy_est, expected),\n",
    "                                        nrmse(da_dasy_est, expected)],            \n",
    "                                'MBE': [mbe(ct_area_est, expected),\n",
    "                                        mbe(da_area_est, expected),\n",
    "                                        mbe(ct_dasy_est, expected),\n",
    "                                        mbe(da_dasy_est, expected)],\n",
    "                                'MAE': [mae(ct_area_est, expected),\n",
    "                                        mae(da_area_est, expected),\n",
    "                                        mae(ct_dasy_est, expected),\n",
    "                                        mae(da_dasy_est, expected)],\n",
    "                                'r': [r_value(ct_area_est, expected),\n",
    "                                        r_value(da_area_est, expected),\n",
    "                                        r_value(ct_dasy_est, expected),\n",
    "                                        r_value(da_dasy_est, expected)],\n",
    "                                'r2': [r_value(ct_area_est, expected)**2,\n",
    "                                        r_value(da_area_est, expected)**2,\n",
    "                                        r_value(ct_dasy_est, expected)**2,\n",
    "                                        r_value(da_dasy_est, expected)**2]}).round(decimals=2)\n",
    "\n",
    "# Display the DataFrame containing the statistics\n",
    "interp_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2551a2a6",
   "metadata": {},
   "source": [
    "From these statistics it's pretty clear that the the dasymetric method is more effective than the area weighted method. This is shown by the lower error values, as represented by the RMSE, NRMSE, and MAE. There's also a higher correlation between the results of the dasymetric method and the neighborhood population estimates.\n",
    "\n",
    "These statistics also show that using the smaller dissemination areas as the source data for the interpolation yields lower errors and higher correlations, compared to the larger census tracts. In fact, using the disseminations areas with the area weighted method even gives better results than the dasymetric method applied to the census tracts.\n",
    "\n",
    "Positive MBE values for all four interpolation results show that all four interpolation results are coming out higher than the neighborhood population estimates. As previously discussed, this is due to a higher total population in the census data (934243) than in the neighborhood data (867146). It's too bad these neighborhood estimates aren't closer to the census data as that would help to make this assessment more credible.\n",
    "\n",
    "Let's see if a visual assessment of the results line up with this statistical one. Plotly offers a very powerful set of plotting tools which render spatial and non-spatial data as interactive figures. There are simpler ways to plot this with less code but the interactive nature of Plotly figures is really great for exploratory data analysis. Before creating the Plotly choropleth facet plots the data needs to be reworked into a specific format. The geometries have be supplied in the GeoJson format and the values have to be melted into a single column with an additional column providing labels.\n",
    "\n",
    "Check out these links for information on the Pandas `df.melt()` method and Plotly choropleth and facet plots:\n",
    "\n",
    "* [`df.melt()` documentation](https://pandas.pydata.org/docs/reference/api/pandas.melt.html)\n",
    "* [Plotly Choropleth Maps](https://plotly.com/python/choropleth-maps/)\n",
    "* [`px.choropleth()` documentation](https://plotly.github.io/plotly.py-docs/generated/plotly.express.choropleth.html)\n",
    "* [Plotly Facet Plots](https://plotly.com/python/facet-plots/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01e4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Percent Error of the four results using a Plotly Facet Map\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# Convert interp_results_gdf from GeoDataFrame to GeoJson\n",
    "geojson = interp_results_gdf.to_crs(epsg='4326').to_json()\n",
    "geojson = json.loads(geojson)\n",
    "\n",
    "# Reformat the interp_results_gdf for the plot\n",
    "df = pd.DataFrame(data=interp_results_gdf.drop(columns='geometry'))\n",
    "\n",
    "# Rename the results columns as they will become the facet plot labels\n",
    "df = df.rename(columns={'ct_area_interp_%_error':'Source: Census Tracts <br> Method: Area Weighted',\n",
    "                    'ct_dasy_interp_%_error':'Source: Census Tracts <br> Method: Dasymetric',\n",
    "                    'da_area_interp_%_error':'Source: Dissemination Areas <br> Method: Area Weighted',\n",
    "                    'da_dasy_interp_%_error':'Source: Dissemination Areas <br> Method: Dasymetric',})\n",
    "\n",
    "# Combine all the results columns into one column labeled with a method column\n",
    "df = df.melt(id_vars='nbhd_name',\n",
    "            value_vars=['Source: Census Tracts <br> Method: Area Weighted',\n",
    "                        'Source: Census Tracts <br> Method: Dasymetric',\n",
    "                        'Source: Dissemination Areas <br> Method: Area Weighted',\n",
    "                        'Source: Dissemination Areas <br> Method: Dasymetric'],\n",
    "            var_name='method', \n",
    "            value_name='Error (%)')\n",
    "\n",
    "# Create the Plotly Express choropleth figure\n",
    "fig = px.choropleth(data_frame=df,\n",
    "                    title=\"Areal Interpolation of 2016 Census Population Data to Ottawa, ON Neighborhoods\",\n",
    "                    locations='nbhd_name',\n",
    "                    geojson=geojson,\n",
    "                    featureidkey='properties.nbhd_name',\n",
    "                    color='Error (%)',\n",
    "                    facet_col='method',\n",
    "                    facet_col_wrap=2,\n",
    "                    hover_name='nbhd_name',\n",
    "                    range_color=[0,100],\n",
    "                    color_continuous_scale='Inferno',\n",
    "                    #color_discrete_sequence= px.colors.sequential.Plasma,\n",
    "                    projection='mercator',\n",
    "                    fitbounds=\"locations\",\n",
    "                    height=800, width=800)\n",
    "\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split('=')[-1]))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db11a9",
   "metadata": {},
   "source": [
    "That's a pretty slick set of maps, if I can say so myself! Each can be explored by zooming in/out and by hovering the cursor over different features. \n",
    "\n",
    "There are some other limitations with Plotly though. While it does support tiled base maps through their MapBox figures (e.g., `px.choropleth_mapbox()`) it's not possible to create facet plots (i.e., side-by-side plots) with a tiled base map (or at least I can't figure out how to do it). A super easy alternative for assessing a single map is GeoPandas' `.explore()` method. Also, Plotly doesn't make it easy to represent the values as discrete classified bins instead of a continuous color scale - no doubt there's a way to do this but we'll have to leave that out for now. Instead I've limited the `Error (%)` color bar range to 100 despite some neighborhoods have percent errors higher than 100. Feel free to adjust the `rangecolor=` argument to a different scale. if you so desire.\n",
    "\n",
    "Another way to represent the results in an interactive map is through GeoPandas' `explore()` method. Like the Plotly maps, it's possible to zoom and inspect the attributes of each feature hovering the cursor over them but we can't put several maps into the same figure.\n",
    "\n",
    "In the cell below we can change the `column` argument to display a color-scaled representation of any of the columns in the `interp_results_gdf`. The classification scheme can also be changed using the `scheme` argument. See the [`gdf.explore()` documentation](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html) for the classification schemes and other parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ca31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually Assess Percent Error of the Area Weighted Interpolation\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# Uncomment one of the following percent error columns to display that its values in the map\n",
    "column = 'ct_area_interp_%_error' # Source: Census Tracts | Method: Area Weighted\n",
    "#column='da_area_interp_%_error' # Source: Dissemination Areas | Method: Area Weighted\n",
    "#column='ct_dasy_interp_%_error' # Source: Census Tracts | Method: Dasymetric\n",
    "#column='da_dasy_interp_%_error' # Source: Dissemination Areas | Method: Dasymetric\n",
    "\n",
    "interp_results_gdf.explore(column=column,\n",
    "    cmap='YlOrRd',\n",
    "    scheme='JenksCaspall',\n",
    "    legend_kwds={'colorbar':False},\n",
    "    style_kwds={'weight':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9faac45",
   "metadata": {},
   "source": [
    "Lastly, let's plot the interpolation results against the neighborhood population estimates using 1:1 scatter plots. We'll use Plotly again for this as its `px.scatter()` facet plots allow us to hover the cursor over the points in order to see their names and other information. It's a lot of code but I think it's worth it.\n",
    "\n",
    "Check out these links for information on Plotly scatter plots:\n",
    "\n",
    "* [Plotly Scatter Plots](https://plotly.com/python/line-and-scatter/)\n",
    "* [`px.scatter()` documentation](https://plotly.com/python-api-reference/generated/plotly.express.scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68824b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1:1 Facet Plots using Plotly\n",
    "# -------------------------------------\n",
    "\n",
    "# Convert the interpolation results to a DataFrame\n",
    "df = pd.DataFrame(data=interp_results_gdf.drop(columns='geometry'))\n",
    "\n",
    "# Rename the results columns as they will become the facet plot labels\n",
    "df = df.rename(columns={'ct_area_interp_est':'Source: Census Tracts <br> Method: Area Weighted',\n",
    "                    'ct_dasy_interp_est':'Source: Census Tracts <br> Method: Dasymetric',\n",
    "                    'da_area_interp_est':'Source: Dissemination Areas <br> Method: Area Weighted',\n",
    "                    'da_dasy_interp_est':'Source: Dissemination Areas <br> Method: Dasymetric'})\n",
    "\n",
    "# Combine all the results columns into one column\n",
    "df = df.melt(id_vars=['nbhd_name', 'nbhd_pop_est'],\n",
    "            value_vars=['Source: Census Tracts <br> Method: Area Weighted',\n",
    "                        'Source: Census Tracts <br> Method: Dasymetric',\n",
    "                        'Source: Dissemination Areas <br> Method: Area Weighted',\n",
    "                        'Source: Dissemination Areas <br> Method: Dasymetric'],\n",
    "            var_name='method', \n",
    "            value_name='interp_pop_est')\n",
    "\n",
    "# Add a percent error column\n",
    "estimated = df['interp_pop_est']\n",
    "expected = df['nbhd_pop_est']\n",
    "df['%_error'] = round(percent_error(estimated, expected), 1)\n",
    "\n",
    "# Create the Plotly Express Scatter figure\n",
    "fig = px.scatter(data_frame=df,\n",
    "                title=\"Areal Interpolation of 2016 Census Population Data to Ottawa, ON Neighborhoods\",\n",
    "                x='nbhd_pop_est',\n",
    "                y='interp_pop_est',\n",
    "                height=800, width=800,\n",
    "                color='%_error',\n",
    "                facet_col='method',\n",
    "                facet_col_wrap=2,\n",
    "                hover_name='nbhd_name',\n",
    "                labels={'interp_pop_est': 'Interpolated Population',\n",
    "                        'nbhd_pop_est': ' Estimated Population'},\n",
    "                color_continuous_scale='Inferno',\n",
    "                range_color=[0,100])\n",
    "\n",
    "# Create the 1:1 line\n",
    "line_max = max([max(df['interp_pop_est']), max(df['nbhd_pop_est'])])\n",
    "line_min = min([min(df['interp_pop_est']), min(df['nbhd_pop_est'])])\n",
    "\n",
    "fig.update_layout(shapes=[\n",
    "        dict(type='line', xref='x', yref='y',\n",
    "            x0=line_min, y0=line_min, x1=line_max, y1=line_max, line_width=1),\n",
    "        dict(type='line', xref='x2', yref='y2',\n",
    "            x0=line_min, y0=line_min, x1=line_max, y1=line_max, line_width=1),\n",
    "        dict(type='line', xref='x3', yref='y3',\n",
    "            x0=line_min, y0=line_min, x1=line_max, y1=line_max, line_width=1),\n",
    "        dict(type='line', xref='x4', yref='y4',\n",
    "            x0=line_min, y0=line_min, x1=line_max, y1=line_max, line_width=1)])\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split('=')[-1]))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a58385",
   "metadata": {},
   "source": [
    "From these plots and maps we can see some shared characteristics of the results of all four combinations of source data and interpolation methods. \n",
    "\n",
    "* They all struggle with the Greenbelt neighborhood. This might be because it's quite large and borders many other neighborhood, or perhaps the census population data and the neighborhood population estimates diverge significantly over this area.\n",
    "* Neighborhoods with small populations, such as Carleton University, Beechwood Cemetary, Notre-Dame Cemetary, Lebreton Development) show high percent errors, possibly due to having low populations while being surrounded by neighborhoods with high populations.\n",
    "* Neighborhoods with high populations, such as Stittsville, Centretown, Old Barrhaven East and \"Bridlewood - Emerald Meadows\" have interpolated populations that are consistently higher than the neighbourhood population estimates. Again, this might be due to underlying issues with the neighbourhood population estimates.\n",
    "* \"Stonebridge - Half Moon Bay - Heart's Desire\" has a high percent error in all results - possibly due to the same issue as above.\n",
    "\n",
    "From this one analysis, and again bearing in mind the issues with the neighborhood population estimates, it's pretty clear that the dasymetric interpolation methods is superior to the area weighted method but that using finer resolution source data has a more powerful contribution to the accuracy of the results.\n",
    "\n",
    "So how can we eliminate the issues with the Ottawa Neighborhood Study population estimates and perform a more objective evaluation of the methods? Well, we can make our own population data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf921f",
   "metadata": {},
   "source": [
    "## Part 2 - Areal Interpolation of Synthetic Population Data\n",
    "\n",
    "In part 2 we will perform the same set of areal interpolations but this time the real population data will be replaced with synthetic population data. By doing this we will know the exact population of the census tracts, dissemination areas, and neighborhoods - allowing us to accurately compare the interpolated populations against the expected populations. A better picture should emerge of the differences between the area weighted and dasymetric interpolation methods, and the effect of the source data's scale.\n",
    "\n",
    "Our steps in part 2 will be:\n",
    "1. Generate synthetic population\n",
    "2. Count the synthetic population\n",
    "3. Perform interpolation\n",
    "4. Assess the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ccf52",
   "metadata": {},
   "source": [
    "### Generate Synthetic Population Data\n",
    "\n",
    "We first need to generate a dataset containing a synthetic population. Ideally, we would have access to a dataset where every person in Ottawa is identified as a point at their dwelling but, because StatCan doesn't release this fine scale for privacy reasons, we will have to approximate it.\n",
    "\n",
    "What we want to do is create a distribution of points that does a pretty good job of approximating the actual distribution of people within a city. It has to have a non-uniform density and be spatially limited to those areas where people actually live. Datasets that could help to approximate this include city land use zones, building footprints, and high resolution land cover maps. To keep things relatively simple here we will just do the following:\n",
    "\n",
    "1. Clip the dissemination areas to the urban land cover type\n",
    "   * Tobler provides a handy function that extracts specific land cover types from a raster and then outputs a GeoDataFrame containing their extent: [`tobler.dasymetric.extract_raster_features()`](https://pysal.org/tobler/generated/tobler.dasymetric.extract_raster_features.html)\n",
    "2. Clip the clipped dissemination areas to Ottawa's land use zones which permit dwellings\n",
    "   * These land use zones come from [Section 35 of the City of Ottawa's zoning definitions](https://ottawa.ca/en/living-ottawa/laws-licences-and-permits/laws/law-z/planning-development-and-construction/maps-and-zoning/zoning-law-no-2008-250/zoning-law-2008-250-consolidation/part-1-administration-interpretation-and-definitions-sections-1-54#section-29-46-interpreting-zoning-information)\n",
    "   * Nineteen zones where dwelling units are permitted have been selected (see the code cell below)\n",
    "   * A more rigorous approach would consider all of the exceptions granted in the subzones but those exceptions will be ignored here\n",
    "3. Generate a number of randomly distributed points within each dissemination area where the number of those points are based off of the 2016 census population\n",
    "4. Count the number of points that fall within each census tract, dissemination area, and neighborhood\n",
    "\n",
    "Generating a reasonably large population of synthetic points within these clipped dissemination areas can take quite a long time (about 15 mins for 93433 points on my old MacBook Pro). If you want to avoid the processing time just run the cell immediately below this one (\"Skip the point generation and just read them from a file\") and then jump ahead to the cell called \"Count the points in each census tract\". Or, if you want to make your own points then skip the next cell and run all of the others.\n",
    "\n",
    "Ok, let's get to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f439274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the point generation and just read them from a file\n",
    "# --------------------------------------------------------\n",
    "\n",
    "points_gdf = gpd.read_file('data/points.gpkg')\n",
    "\n",
    "# Optional: uncomment the line below to explore the points\n",
    "#points_gdf.explore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35ee156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clipped dissemination areas\n",
    "# ----------------------------------\n",
    "\n",
    "# Extract the urban landcover (pixel value of 17) from the landcover raster\n",
    "raster_path = 'data/ottawa_landcover.tif'\n",
    "urban_landcover_gdf = tobler.dasymetric.extract_raster_features(gdf=nbhd_gdf,\n",
    "                                                                raster_path=raster_path,\n",
    "                                                                pixel_values=17)\n",
    "\n",
    "urban_landcover_gdf = urban_landcover_gdf.to_crs(epsg=32618)\n",
    "\n",
    "# Read the Ottawa zoning\n",
    "zoning_gdf = gpd.read_file('data/ottawa_zoning.gpkg')\n",
    "\n",
    "# Only keep the zones that residents can live in\n",
    "people_zones = ['R1', 'R2', 'R3', 'R4', 'R5', 'RM', 'LC', 'GM', 'TM',\n",
    "                'AM','MC','MD', 'AG', 'RR','RU','VM', 'V1','V2','V3']\n",
    "\n",
    "zoning_gdf = zoning_gdf[zoning_gdf['zone_main'].isin(people_zones)]\n",
    "\n",
    "# Clip the dissemination areas to the urban landcover and the zoning\n",
    "da_clip_gdf = gpd.clip(gdf=da_gdf, mask=zoning_gdf)\n",
    "da_clip_gdf = gpd.clip(gdf=da_clip_gdf, mask=urban_landcover_gdf)\n",
    "\n",
    "# ~2 mins ; ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c306fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate a specific number of random points in a polygon\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def random_points(row, n_column, geom_column, divisor=1, min_n=1):\n",
    "    ''' Returns n number of random points within GeoDataFrame polygons.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    row : Pandas Series\n",
    "        Must contain:\n",
    "        - A column containing the desired number of points in each polygons\n",
    "        - A column containing polygon geometry\n",
    "\n",
    "    n_column : string\n",
    "        - The name of the column that contains the desired number of points\n",
    "\n",
    "    geom_column : string\n",
    "        - The name of the column that contains the GeoDataBase geometry\n",
    "\n",
    "    divisor : int (default = 1)\n",
    "        - A value used to reduce the number of points (n2 = n1 / divisor)\n",
    "        - Good for shortening computation time\n",
    "\n",
    "    min_n : int (default = 1)\n",
    "        - The minimum number of points in polygon\n",
    "        - Good for forcing all polygons to have at least 1 point\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    This function is very slow when trying to generate points in polygons that\n",
    "    have coverages that are much smaller than their extent (e.g a C-shaped polygon)\n",
    "    '''\n",
    "    \n",
    "    # Read the polygon and its bounds\n",
    "    polygon = row[geom_column]\n",
    "    min_x, min_y, max_x, max_y = polygon.bounds\n",
    "\n",
    "    # Determine the number of points to generate in the polygon\n",
    "    n_points = round((row[n_column] / divisor), 0)\n",
    "    if n_points < min_n:\n",
    "        n_points = min_n\n",
    "    \n",
    "    # Generate the points and put them in a list\n",
    "    points_list = []\n",
    "    while len(points_list) < n_points:\n",
    "        point = Point(np.random.uniform(min_x, max_x), np.random.uniform(min_y, max_y))\n",
    "        if point.intersects(polygon):\n",
    "            points_list.append(point)\n",
    "\n",
    "    return points_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78988b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate points in the clipped dissemination areas\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Generate the random points\n",
    "points_srs = da_clip_gdf.apply(func=random_points, \n",
    "                            args=('da_pop_2016', 'geometry', 10, 1), axis=1)\n",
    "\n",
    "# The output is a pd.Series so convert it to a list\n",
    "points_list = list(points_srs)\n",
    "\n",
    "# Flatten the list of lists so each item in the list is a point\n",
    "points_flat_list = []\n",
    "for i in points_list:\n",
    "    for j in i:\n",
    "        points_flat_list.append(j)\n",
    "\n",
    "# Create a GeoDataFrame of the synthetic points using the flat list as the geometry column\n",
    "points_gdf = gpd.GeoDataFrame(geometry=points_flat_list, crs=\"EPSG:32618\")\n",
    "\n",
    "# If you made new points and want to save them,\n",
    "# uncomment the line below to write them to a file\n",
    "#points_gdf.to_file('data/new_points_gdf.gpkg', driver='GPKG')\n",
    "\n",
    "# Optional: uncomment the line below to explore the points\n",
    "#points_gdf.explore()\n",
    "\n",
    "# ~15-20 mins if divisor=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe66793c",
   "metadata": {},
   "source": [
    "We now have a GeoDataFrame containing the synthetic population (`points_gdf`). Each point is a synthetic person that lives where dwellings are allowed in the urban land cover class.\n",
    "\n",
    "We can count how many fall into each census tract, dissemination area, and neighborhood by using the GeoPandas `gpd.sjoin()` spatial join method, giving each point a value of 1, grouping the features by their id or name, and them summing them. To see how exactly this works feel free to split the cells below into single expressions and then use `gdf.head()` on the intermediate GeoDataFrames.\n",
    "\n",
    "For more information on these operations check out the documentation:\n",
    "* [`gpd.sjoin()`](https://geopandas.org/en/stable/docs/reference/api/geopandas.sjoin.html)\n",
    "* [`pd.groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)\n",
    "* [`pd.groupby.sum()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.sum.html)\n",
    "* [`pd.merge`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the points in each census tract\n",
    "# -------------------------------------\n",
    "\n",
    "# Spatial join of points_gdf to ct_gdf\n",
    "ct_points_gdf = gpd.sjoin(points_gdf, ct_gdf, how='inner', predicate='intersects')\n",
    "\n",
    "# Count the points that fall in each census tract\n",
    "ct_points_gdf['ct_synth_pop'] = 1\n",
    "ct_points_sums_gdf = ct_points_gdf.groupby(['ctuid']).sum()\n",
    "\n",
    "# Merge the counts with the census tracts\n",
    "ct_synth_gdf = ct_gdf.merge(ct_points_sums_gdf[['ct_synth_pop']], on='ctuid')\n",
    "\n",
    "# Print the results\n",
    "print('Total points', ct_synth_gdf['ct_synth_pop'].sum())\n",
    "ct_synth_gdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c40494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the points in each dissemination area\n",
    "# -------------------------------------------\n",
    "\n",
    "# Spatial join of points_gdf to da_gdf\n",
    "da_points_gdf = gpd.sjoin(points_gdf, da_gdf, how='inner', predicate='intersects')\n",
    "\n",
    "# Count the points that fall in each dissemination areas\n",
    "da_points_gdf['da_synth_pop'] = 1\n",
    "da_points_sums_gdf = da_points_gdf.groupby(['dauid']).sum()\n",
    "\n",
    "# Merge the counts with the census tracts\n",
    "da_synth_gdf = da_gdf.merge(da_points_sums_gdf[['da_synth_pop']], on='dauid')\n",
    "\n",
    "# Print the results\n",
    "print('Total points', da_synth_gdf['da_synth_pop'].sum())\n",
    "da_synth_gdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb1c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the points in each neighborhood\n",
    "# -------------------------------------\n",
    "\n",
    "# Spatial join of synthetic points to neighborhoods\n",
    "nbhd_points_gdf = gpd.sjoin(points_gdf, nbhd_gdf, how='inner', predicate='intersects')\n",
    "\n",
    "# Count the points that fall in each neighborhood\n",
    "nbhd_points_gdf['nbhd_synth_pop'] = 1\n",
    "nbhd_points_sums_gdf = nbhd_points_gdf.groupby(['nbhd_name']).sum()\n",
    "\n",
    "# Merge the counts with the neighborhoods\n",
    "nbhd_synth_gdf = nbhd_gdf.merge(nbhd_points_sums_gdf[['nbhd_synth_pop']], on='nbhd_name')\n",
    "\n",
    "# Print the results\n",
    "print('Total points', nbhd_synth_gdf['nbhd_synth_pop'].sum())\n",
    "nbhd_synth_gdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee11279",
   "metadata": {},
   "source": [
    "### Areal Interpolation of Synthetic Data\n",
    "\n",
    "#### Area Weighted Interpolation of Census Tracts and Dissemination Areas\n",
    "\n",
    "The synthetic census tract and dissemination areas population data can now be used as the source for area weighted and dasymetric interpolations. We will repeat all of the steps that we took in Part 1 but with the synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da8e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area weighted interpolation: synthetic census tracts to neighborhoods\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "ct_area_interp_gdf = tobler.area_weighted.area_interpolate(source_df=ct_synth_gdf, \n",
    "                                                        target_df=nbhd_synth_gdf,\n",
    "                                                        extensive_variables=['ct_synth_pop'])\n",
    "\n",
    "# Round the interpolation results to the nearest integer and change the type to integer\n",
    "# (Population counts must be integers)\n",
    "ct_area_interp_gdf['ct_synth_pop'] = ct_area_interp_gdf['ct_synth_pop'].round(decimals=0).astype(int)\n",
    "\n",
    "# Rename the results column for clarity later\n",
    "ct_area_interp_gdf = ct_area_interp_gdf.rename(columns={'ct_synth_pop':'ct_area_interp_est'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68009c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area weighted interpolation: synthetic dissemination areas to neighborhoods\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "da_area_interp_gdf = tobler.area_weighted.area_interpolate(source_df=da_synth_gdf, \n",
    "                                                        target_df=nbhd_synth_gdf,\n",
    "                                                        extensive_variables=['da_synth_pop'])\n",
    "\n",
    "# Round the interpolation results to the nearest integer and change the type to integer\n",
    "# (Population counts must be integers)\n",
    "da_area_interp_gdf['da_synth_pop'] = da_area_interp_gdf['da_synth_pop'].round(decimals=0).astype(int)\n",
    "\n",
    "# Rename the results column for clarity later\n",
    "da_area_interp_gdf = da_area_interp_gdf.rename(columns={'da_synth_pop':'da_area_interp_est'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569fb22",
   "metadata": {},
   "source": [
    "#### Dasymetric Interpolation of Census Tracts and Dissemination Areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560f83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dasymetric interpolation: synthetic census tracts + urban landover to neighborhoods\n",
    "#------------------------------------------------------------------------------------\n",
    "\n",
    "# Perform dasymetric interpolation\n",
    "ct_dasy_interp_gdf = tobler.dasymetric.masked_area_interpolate(source_df=ct_synth_gdf, \n",
    "                                                            target_df=nbhd_synth_gdf,\n",
    "                                                            raster='data/ottawa_landcover.tif',\n",
    "                                                            codes=[17],\n",
    "                                                            extensive_variables=['ct_synth_pop'])\n",
    "\n",
    "# Round the interpolation results to the nearest integer and change the type to integer\n",
    "# (Population counts must be integers)\n",
    "ct_dasy_interp_gdf['ct_synth_pop'] = ct_dasy_interp_gdf['ct_synth_pop'].round(decimals=0).astype(int)\n",
    "\n",
    "# Rename the results column for clarity later\n",
    "ct_dasy_interp_gdf = ct_dasy_interp_gdf.rename(columns={'ct_synth_pop':'ct_dasy_interp_est'})\n",
    "\n",
    "# ~30 s ; ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dasymetric interpolation: synthetic dissemination areas + urban landover to neighborhoods\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "# Perform dasymetric interpolation\n",
    "da_dasy_interp_gdf = tobler.dasymetric.masked_area_interpolate(source_df=da_synth_gdf, \n",
    "                                                            target_df=nbhd_synth_gdf,\n",
    "                                                            raster='data/ottawa_landcover.tif',\n",
    "                                                            codes=[17],\n",
    "                                                            extensive_variables=['da_synth_pop'])\n",
    "\n",
    "# Round the interpolation results to the nearest integer and change the type to integer\n",
    "# (Population counts must be integers)\n",
    "da_dasy_interp_gdf['da_synth_pop'] = da_dasy_interp_gdf['da_synth_pop'].round(decimals=0).astype(int)\n",
    "\n",
    "# Rename the results column for clarity later\n",
    "da_dasy_interp_gdf = da_dasy_interp_gdf.rename(columns={'da_synth_pop':'da_dasy_interp_est'})\n",
    "\n",
    "# ~1.5 mins ; ignore warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85635df6",
   "metadata": {},
   "source": [
    "### Assess the Interpolation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the results for comparison\n",
    "#---------------------------------\n",
    "\n",
    "# Create a list of the GeoDataFrames (drop the redundant geometry)\n",
    "dfs = [nbhd_synth_gdf,\n",
    "        ct_area_interp_gdf.drop(columns='geometry'),\n",
    "        ct_dasy_interp_gdf.drop(columns='geometry'),\n",
    "        da_area_interp_gdf.drop(columns='geometry'),\n",
    "        da_dasy_interp_gdf.drop(columns='geometry'),]\n",
    "\n",
    "# Concatenate the GeoDataFrames\n",
    "interp_results_gdf = pd.concat(dfs, axis=1)\n",
    "\n",
    "# View a sample of the interpolation results GeoDataFrame (without the geometry column)\n",
    "interp_results_gdf.sample(10).drop(columns='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89657d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Results\n",
    "# ----------------\n",
    "\n",
    "# Assign the interpolation results columns to their own variables for clarity\n",
    "ct_area_est = interp_results_gdf['ct_area_interp_est'] # Source: census tracts | method: area weighted\n",
    "ct_dasy_est = interp_results_gdf['ct_dasy_interp_est'] # Source: census tracts  method: dasymetric\n",
    "da_area_est = interp_results_gdf['da_area_interp_est'] # Source: dissemination areas | method: area weighted\n",
    "da_dasy_est = interp_results_gdf['da_dasy_interp_est'] # Source: dissemination areas | method: dasymetric\n",
    "expected = interp_results_gdf['nbhd_synth_pop']        # Source: neighborhoods\n",
    "\n",
    "# Percent Error - create new columns in interp_results_gdf\n",
    "interp_results_gdf['ct_area_interp_%_error'] = round(percent_error(ct_area_est, expected), 1)\n",
    "interp_results_gdf['ct_dasy_interp_%_error'] = round(percent_error(ct_dasy_est, expected), 1)\n",
    "interp_results_gdf['da_area_interp_%_error'] = round(percent_error(da_area_est, expected), 1)\n",
    "interp_results_gdf['da_dasy_interp_%_error'] = round(percent_error(da_dasy_est, expected), 1)\n",
    "\n",
    "# Other statistics - create DataFrame of statistics\n",
    "interp_stats_df = pd.DataFrame(data={'Interpolation Method': ['Area Weighted','Area Weighted',\n",
    "                                                        'Dasymetric', 'Dasymetric'],\n",
    "                                'Source Geographies': ['Census Tracts', 'Dissemination Areas',\n",
    "                                                       'Census Tracts', 'Dissemination Areas'],\n",
    "                                'RMSE': [rmse(ct_area_est, expected),\n",
    "                                        rmse(da_area_est, expected),\n",
    "                                        rmse(ct_dasy_est, expected),\n",
    "                                        rmse(da_dasy_est, expected)],\n",
    "                                'NRMSE': [nrmse(ct_area_est, expected),\n",
    "                                        nrmse(da_area_est, expected),\n",
    "                                        nrmse(ct_dasy_est, expected),\n",
    "                                        nrmse(da_dasy_est, expected)],            \n",
    "                                'MBE': [mbe(ct_area_est, expected),\n",
    "                                        mbe(da_area_est, expected),\n",
    "                                        mbe(ct_dasy_est, expected),\n",
    "                                        mbe(da_dasy_est, expected)],\n",
    "                                'MAE': [mae(ct_area_est, expected),\n",
    "                                        mae(da_area_est, expected),\n",
    "                                        mae(ct_dasy_est, expected),\n",
    "                                        mae(da_dasy_est, expected)],\n",
    "                                'r': [r_value(ct_area_est, expected),\n",
    "                                        r_value(da_area_est, expected),\n",
    "                                        r_value(ct_dasy_est, expected),\n",
    "                                        r_value(da_dasy_est, expected)],\n",
    "                                'r2': [r_value(ct_area_est, expected)**2,\n",
    "                                        r_value(da_area_est, expected)**2,\n",
    "                                        r_value(ct_dasy_est, expected)**2,\n",
    "                                        r_value(da_dasy_est, expected)**2]}).round(decimals=2)\n",
    "\n",
    "interp_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca075c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Percent Error of Both Methods using a Plotly Facet Map\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# Convert interp_results_gdf from GeoDataFrame to GeoJson\n",
    "geojson = interp_results_gdf.to_crs(epsg='4326').to_json()\n",
    "geojson = json.loads(geojson)\n",
    "\n",
    "# Reformat the interp_results_gdf for the plot\n",
    "df = pd.DataFrame(interp_results_gdf.drop(columns='geometry'))\n",
    "\n",
    "df = df.rename(columns={'ct_area_interp_%_error':'Source: Census Tracts <br> Method: Area Weighted',\n",
    "                'ct_dasy_interp_%_error':'Source: Census Tracts <br> Method: Dasymetric',\n",
    "                'da_area_interp_%_error':'Source: Dissemination Areas <br> Method: Area Weighted',\n",
    "                'da_dasy_interp_%_error':'Source: Dissemination Areas <br> Method: Dasymetric'})\n",
    "\n",
    "df = df.melt(id_vars='nbhd_name',\n",
    "            value_vars=['Source: Census Tracts <br> Method: Area Weighted',\n",
    "                        'Source: Census Tracts <br> Method: Dasymetric',\n",
    "                        'Source: Dissemination Areas <br> Method: Area Weighted',\n",
    "                        'Source: Dissemination Areas <br> Method: Dasymetric'],\n",
    "            var_name='method', \n",
    "            value_name='Error (%)')\n",
    "\n",
    "# Create the Plotly Express choropleth figure\n",
    "fig = px.choropleth(data_frame=df,\n",
    "                    title=\"Areal Interpolation of Synthetic Population Data to Ottawa, ON Neighborhoods\",\n",
    "                    locations='nbhd_name',\n",
    "                    geojson=geojson,\n",
    "                    featureidkey='properties.nbhd_name',\n",
    "                    color='Error (%)',\n",
    "                    facet_col='method',\n",
    "                    facet_col_wrap=2,\n",
    "                    hover_name='nbhd_name',\n",
    "                    range_color=[0,100],\n",
    "                    color_continuous_scale='Inferno',\n",
    "                    projection='mercator',\n",
    "                    fitbounds=\"locations\",\n",
    "                    height=800, width=800)\n",
    "         \n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split('=')[-1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1:1 Facet Plots using Plotly\n",
    "# -----------------------------------\n",
    "\n",
    "# Convert the interpolation results to a DataFrame\n",
    "df = pd.DataFrame(data=interp_results_gdf.drop(columns='geometry'))\n",
    "\n",
    "# Rename the results columns as they will become the facet plot labels\n",
    "df = df.rename(columns={'ct_area_interp_est':'Source: Census Tracts <br> Method: Area Weighted',\n",
    "                'ct_dasy_interp_est':'Source: Census Tracts <br> Method: Dasymetric',\n",
    "                'da_area_interp_est':'Source: Dissemination Areas <br> Method: Area Weighted',\n",
    "                'da_dasy_interp_est':'Source: Dissemination Areas <br> Method: Dasymetric'})\n",
    "\n",
    "# Combine all the results columns into one column\n",
    "df = df.melt(id_vars=['nbhd_name', 'nbhd_synth_pop'],\n",
    "            value_vars=['Source: Census Tracts <br> Method: Area Weighted',\n",
    "                        'Source: Census Tracts <br> Method: Dasymetric',\n",
    "                        'Source: Dissemination Areas <br> Method: Area Weighted',\n",
    "                        'Source: Dissemination Areas <br> Method: Dasymetric'],\n",
    "            var_name='method', \n",
    "            value_name='interp_pop_est')\n",
    "\n",
    "# Add a percent error column\n",
    "estimated = df['interp_pop_est']\n",
    "expected = df['nbhd_synth_pop']\n",
    "df['%_error'] = round(percent_error(estimated, expected), 1)\n",
    "\n",
    "# Create the Plotly Express Scatter figure\n",
    "fig = px.scatter(data_frame=df,\n",
    "                title=\"Areal Interpolation of Synthetic Population Data to Ottawa, ON Neighborhoods\",\n",
    "                x='nbhd_synth_pop',\n",
    "                y='interp_pop_est',\n",
    "                height=800, width=800,\n",
    "                color='%_error',\n",
    "                facet_col='method',\n",
    "                facet_col_wrap=2,\n",
    "                hover_name='nbhd_name',\n",
    "                labels={'interp_pop_est': 'Interpolated Population',\n",
    "                        'nbhd_synth_pop': ' Estimated Population'},\n",
    "                color_continuous_scale='Inferno',\n",
    "                range_color=[0,100])\n",
    "\n",
    "# Create the 1:1 line\n",
    "line_max = max([max(df['interp_pop_est']), max(df['nbhd_synth_pop'])])\n",
    "line_min = min([min(df['interp_pop_est']), min(df['nbhd_synth_pop'])])\n",
    "\n",
    "fig.update_layout(shapes=[\n",
    "        dict(type='line', xref='x', yref='y',\n",
    "            x0=line_min, y0=line_min, x1=line_max, y1=line_max, line_width=1),\n",
    "        dict(type='line', xref='x2', yref='y2',\n",
    "            x0=line_min, y0=line_min, x1=line_max, y1=line_max, line_width=1),\n",
    "        dict(type='line', xref='x3', yref='y3',\n",
    "            x0=line_min, y0=line_min, x1=line_max, y1=line_max, line_width=1),\n",
    "        dict(type='line', xref='x4', yref='y4',\n",
    "            x0=line_min, y0=line_min, x1=line_max, y1=line_max, line_width=1)])\n",
    "\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split('=')[-1]))\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
